{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430239d6",
   "metadata": {},
   "source": [
    "Project overview and goals: \n",
    "In this project we explore the training set of the Netflix prize dataset. The Netflix Prize was an open competition for the best collaborative filtering algorithm to predict user ratings for films, based on previous rating (Wikipedia). The dataset reports more than one million of ratings of netflix titles over the year. We perform descriptive statistics and create different data visualisation. It is an exploratory study. Our goal is to answer simple questions concerning the data, i.e. : \n",
    "- how ratings are overall distributed? and how are they distributed over the years? \n",
    "- which are the most liked movies in the database? \n",
    "- how rating interact with movie genre? \n",
    "- can we get an idea of rating habits of the customers depending on their activity level?\n",
    "\n",
    "Materials: \n",
    "The netflix database was divided in 4 .txt files containing a first row with movie id and under movie id, customer id, rating, and date. Because of the size of the database, we created a random sample of 500.000 lines in the script sampling.py, adjusting also the structure of the data to have the following columns: movie id, customer id, rating, date. We added also a file containing the movie titles and year of production, and a file containing movie genres. This database has been chosen for the richness of its data and for being a well-known and widely used database.\n",
    "\n",
    "Methods: \n",
    "- In the DATA MANAGEMENT part of the notebook, we 1) load the data and merge the dfs to create the final dataframe; 2) explore the dataset and deal with missing data; 3) create the new variables\n",
    "- In the DATA EPLORATION part, we perform descriptive statistsics \n",
    "- In the DATA VISUALIZATION part, we created the different plots \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c0784f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ec9c7b",
   "metadata": {},
   "source": [
    "DATA MANAGEMENT PART \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceace51a",
   "metadata": {},
   "source": [
    "Step 1: load ratings data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0eb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings = pd.read_csv(\"netflix_sampled_500k_proportional.csv\")\n",
    "\n",
    "\n",
    "df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47f843",
   "metadata": {},
   "source": [
    "Step 2. Load and join the movie titles file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f9c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read and clean the movie titles file \n",
    "titles_rows = []\n",
    "    \n",
    "with open('movie_titles.csv', 'r', encoding = 'latin-1') as f: \n",
    "    for line in f: \n",
    "      line = line.strip()\n",
    "      parts = line.split(',', 2)\n",
    "\n",
    "      if len(parts) == 3:\n",
    "            film_id, year, title = parts\n",
    "            titles_rows.append([int(film_id), year, title])\n",
    "\n",
    "df_titles = pd.DataFrame(titles_rows, columns=['movie_id', 'year', 'title'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa80d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82254298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the movie titles file with the rating file\n",
    "\n",
    "df_ratings_titles   = pd.merge(df_ratings, df_titles, how = 'left', on = 'movie_id')\n",
    "\n",
    "\n",
    "\n",
    "df_ratings_titles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511bee14",
   "metadata": {},
   "source": [
    "Step 3. Load and join the genres file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489abf93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the genre file\n",
    "df_genres = pd.read_csv('netflix_genres.csv')\n",
    "\n",
    "df_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857e0efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the df with the genre file \n",
    "\n",
    "df = pd.merge(df_ratings_titles, df_genres, how = 'left', left_on = 'movie_id', right_on = 'movieId').drop('movieId', axis = 1)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2676452",
   "metadata": {},
   "source": [
    "Step 4. Data exploration, missing data treatment and variable creation \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd2ef01",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Première choses dans l'analyse exploratoire: Connaître le Dataframe\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131b32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### change type for the year and date variables which are strings \n",
    "\n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['year'] = pd.to_numeric(df['year'], errors='coerce')\n",
    "df.info() ### On voit bien que maintenant tout est dans le bon format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f46a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check for missing data \n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ae4e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have missing data for the year so let's see what they are \n",
    "df.loc[(df['year'].isna())]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9571497",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's drop them for now (@Paulo being only three titles we could fill them with the real values, opinions? )\n",
    "df = df.dropna(subset={\"year\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d131a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the na again \n",
    "\n",
    "df.isnull().sum()\n",
    "\n",
    "#year is good now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331927ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have missing data in the genre column so let's visualize them \n",
    "df.loc[(df['genres'].isna())]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990d41fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill the NaN with 'Empty' (@Paulo: for now, tell me if you have other ideas for missing values)\n",
    "\n",
    "df['genres'].fillna('Empty', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158e0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d35c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i want to see how many of these are tv series and not movies, documentaries etc\n",
    " \n",
    "\n",
    "#filter the titles that contain 'season '. This is the most common structure of the title when it is a series but some others could be left out, let's just have a look\n",
    "series = df['title'].str.contains('Season ')\n",
    "\n",
    "\n",
    "\n",
    "print(series.sum())\n",
    "\n",
    "\n",
    "df[series]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547e469b",
   "metadata": {},
   "source": [
    "Variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c14c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's start with the new variables \n",
    "\n",
    "#the first column we add the decade\n",
    "# it is a conditional column: if 1980<=year=>1989 -> 1980s \n",
    "\n",
    "\n",
    "#let's check the min and max \n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f6f812",
   "metadata": {},
   "outputs": [],
   "source": [
    "###on voit que year min = 1896 et max= 2005 donc\n",
    "conditions = [\n",
    "    (df[\"year\"] >= 1890) & (df[\"year\"] <= 1899),\n",
    "    (df[\"year\"] >= 1900) & (df[\"year\"] <= 1909),\n",
    "    (df[\"year\"] >= 1910) & (df[\"year\"] <= 1919),\n",
    "    (df[\"year\"] >= 1920) & (df[\"year\"] <= 1929),\n",
    "    (df[\"year\"] >= 1930) & (df[\"year\"] <= 1939),\n",
    "    (df[\"year\"] >= 1940) & (df[\"year\"] <= 1949),\n",
    "    (df[\"year\"] >= 1950) & (df[\"year\"] <= 1959),\n",
    "    (df[\"year\"] >= 1960) & (df[\"year\"] <= 1969),\n",
    "    (df[\"year\"] >= 1970) & (df[\"year\"] <= 1979),\n",
    "    (df[\"year\"] >= 1980) & (df[\"year\"] <= 1989),\n",
    "    (df[\"year\"] >= 1990) & (df[\"year\"] <= 1999),\n",
    "    (df[\"year\"] >= 2000) & (df[\"year\"] <= 2005)\n",
    "]\n",
    "\n",
    "values = [\n",
    "    \"1890s\",\n",
    "    \"1900s\",\n",
    "    \"1910s\",\n",
    "    \"1920s\",\n",
    "    \"1930s\",\n",
    "    \"1940s\",\n",
    "    \"1950s\",\n",
    "    \"1960s\",\n",
    "    \"1970s\",\n",
    "    \"1980s\",\n",
    "    \"1990s\",\n",
    "    \"2000s\"\n",
    "]\n",
    "\n",
    "df[\"decade\"] = np.select(conditions, values, default=\"Out of Range\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967cab4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2e07c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add another variable: rating category \n",
    "\n",
    "# Define the conditions for each category\n",
    "rating_conditions = [\n",
    "    (df['rating'] <= 2),\n",
    "    (df['rating'] == 3),\n",
    "    (df['rating'] >= 4)\n",
    "]\n",
    "\n",
    "# Define the corresponding values for each category\n",
    "categories = ['Low', 'Neutral', 'High']\n",
    "\n",
    "# Create the new column using np.select\n",
    "df['rating_category'] = np.select(rating_conditions, categories, default='Unknown')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6f3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add another variable: divide users based on activity levels \n",
    "\n",
    "\n",
    "#we create a df customer stats to group by customers and have the number of ratings per customer \n",
    "\n",
    "\n",
    "customer_stats = df.groupby('customer_id').agg(\n",
    "    # 1. Calculate the number of ratings (Rating Volume)\n",
    "    num_ratings=('rating', 'count'),\n",
    "    # 2. Calculate the average rating (Rating Tendency)\n",
    "    avg_rating=('rating', 'mean')\n",
    ").reset_index()\n",
    "\n",
    "# Step 1: We try to divide the customers based on quintiles and check how many categories have been created \n",
    "cut_results, bins = pd.qcut(\n",
    "    customer_stats['num_ratings'],\n",
    "    q=5, \n",
    "    labels=False,  # Temporarily prevents the error\n",
    "    duplicates='drop',\n",
    "    retbins=True \n",
    ")\n",
    "\n",
    "num_bins_created = len(bins) - 1\n",
    "print(f\"You have {num_bins_created} unique bins.\")\n",
    "\n",
    "# based on the number of created bins we create the categories \n",
    "\n",
    "customer_stats['activity_level'] = pd.qcut(\n",
    "    customer_stats['num_ratings'],\n",
    "    q=5, # Request 5 groups, but pandas only makes 3 due to duplicates='drop'\n",
    "    labels=['Low', 'Medium', 'High'], # <-- Must use only 3 labels\n",
    "    duplicates='drop' \n",
    ")\n",
    "\n",
    "\n",
    "print(customer_stats['activity_level'].value_counts())\n",
    "\n",
    "\n",
    "# Now we merge the new 'activity_level' column from customer_stats into the main df\n",
    "df = pd.merge(\n",
    "    df,\n",
    "    customer_stats[['customer_id', 'activity_level']],\n",
    "    on='customer_id',\n",
    "    how='left' # Ensure all rows from df are kept\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45261330",
   "metadata": {},
   "source": [
    "Describe dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3ada57",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee03a8f",
   "metadata": {},
   "source": [
    "Subsetting and groupings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466ace0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grouping of decades and rating \n",
    "\n",
    "df_view_decades = (\n",
    "    df[[\"decade\", \"rating\"]]\n",
    "    .groupby(\"decade\", as_index=False)\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d125bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Here we calculate a weighted rating depending on number of ratings per customer and obtain the 10 top rated movies \n",
    "\n",
    "# average movie rating and rating count \n",
    "\n",
    "movie_stats = df.groupby(['movie_id', 'title']).agg(\n",
    "    rating_count=('rating', 'count'), \n",
    "    # This calculates the average rating (R) for the WR formula\n",
    "    avg_rating=('rating', 'mean') \n",
    ").reset_index()\n",
    "\n",
    "\n",
    "#calculate overall average rating \n",
    "C = df['rating'].mean()\n",
    "\n",
    "# 2. Calculate m (Minimum Votes Threshold - here we use the 90th percentile) \n",
    "m = movie_stats['rating_count'].quantile(0.90)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns for the calculation of the weighted rating \n",
    "v = movie_stats['rating_count']\n",
    "R = movie_stats['avg_rating']\n",
    "\n",
    "# Calculate WR using the constants C and m:\n",
    "movie_stats['weighted_rating'] = (\n",
    "    (v / (v + m)) * R \n",
    ") + (\n",
    "    (m / (v + m)) * C\n",
    ")\n",
    "\n",
    "#define the movies that have higher rating count than the threshold \n",
    "eligible_movies = movie_stats[movie_stats['rating_count'] >= m].copy()\n",
    "\n",
    "# 2. Sort by the Weighted Rating (WR) and take the first 10 movies \n",
    "top_movies = eligible_movies.sort_values(by='weighted_rating', ascending=False).head(10)\n",
    "\n",
    "print(top_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3d19cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all data to use in exploration.py \n",
    "\n",
    "output_dir = 'data'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "\n",
    "df.to_csv(os.path.join(output_dir, 'main_df.csv'), index=False)\n",
    "movie_stats.to_csv(os.path.join(output_dir, 'movies_by_rating.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
